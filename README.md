# This Repo is for my Masters Project

## Title : Performance Comparison of BERT, DistilBERT, ALBERT, TinyBERT, and MiniLM on the Efficient Question Answering


## Overview
This project compares the performance of BERT, DistilBERT, ALBERT, TinyBERT, and MiniLM on the SQuAD dataset. The SQuAD dataset is a large-scale dataset for question answering, and it is used to evaluate the performance of these models.

## Models Used

- **BERT**  
- **DistilBERT**  
- **ALBERT**  
- **TinyBERT**  
- **MiniLM** 


## Installation
To install the project, follow these steps:

Download the notebooks and use google colab or any other similar one.

## Dependencies
To run this project, you'll need the following libraries:
- Python 3.x
- HuggingFace Transformers
- Datasets

You can install the required libraries with the following command:
```bash
pip install transformers datasets evaluate

```
## Expected Outcomes

This study is expected to:
- Highlight the performance differences among BERT, DistilBERT, ALBERT, TinyBERT, and MiniLM.  
- Demonstrate the impact of model size and architecture on QA performance.  
- Provide recommendations for model selection based on specific use cases.
